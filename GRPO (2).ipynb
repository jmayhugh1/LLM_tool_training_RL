{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnG-m5CMWRsp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/rl_llms')\n",
        "# !pip install -U trl\n",
        "\n",
        "!pip install -U bitsandbytes\n",
        "!pip install wandb\n",
        "!pip install trl[vllm]\n",
        "import trl\n",
        "import wandb\n",
        "wandb_api_key = \"UR wandb api key\"\n",
        "wandb.login(key = wandb_api_key)\n",
        "print(trl.__version__)\n",
        "print('Google Drive mounted and set as current working directory.')\n",
        "!ls\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X_a-QIjQPIwg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd8LRWC_eAhd"
      },
      "source": [
        "# Loading in model and datasets\n",
        "\n",
        "Here we load in the previous fine-tuned qwen model, and the corresponding tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OEsrg_gWXV9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import torch\n",
        "ckpt_dir  = \"Qwen2_0_5B_Instruct_2025-11-21 20:42:36_xLAM_2/checkpoint-250\"\n",
        "HF_TOKEN = \"UR HF token\"\n",
        "MODEL_NAME = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.padding_side = \"left\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    ckpt_dir,\n",
        "    torch_dtype=torch.bfloat16,  # or float16 / float32 depending on what you want\n",
        "    device_map=\"auto\",\n",
        "    token = HF_TOKEN,\n",
        ")\n",
        "\n",
        "gen_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "fine_tuned_model.generation_config = gen_config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Training Configs\n",
        "\n",
        "These training configs were the ones from Behrooz Azarkhali and were held consistent with the SFT.\n",
        "\n",
        " https://huggingface.co/learn/cookbook/en/function_calling_fine_tuning_llms_on_xlam\n",
        "\n"
      ],
      "metadata": {
        "id": "GDoLwbXaPTQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk8RO1Cgd8ee"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import time\n",
        "from peft import LoraConfig\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for model-specific settings.\"\"\"\n",
        "    model_name: str           # HuggingFace model identifier\n",
        "    pad_token: str           # Padding token for the tokenizer\n",
        "    pad_token_id: int        # Numerical ID for the padding token\n",
        "    padding_side: str        # Side to add padding ('left' or 'right')\n",
        "    eos_token: str          # End of sequence token\n",
        "    eos_token_id: int       # End of sequence token ID\n",
        "    vocab_size: int         # Vocabulary size\n",
        "    model_type: str         # Model architecture type\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Configuration for training hyperparameters.\"\"\"\n",
        "    output_dir: str                    # Directory to save model checkpoints\n",
        "    batch_size: int = 16              # Training batch size per device\n",
        "    gradient_accumulation_steps: int = 8  # Steps to accumulate gradients\n",
        "    learning_rate: float = 1e-4       # Learning rate for optimization\n",
        "    max_steps: int = 1000             # Maximum training steps\n",
        "    max_seq_length: int = 2048        # Maximum sequence length\n",
        "    lora_r: int = 16                  # LoRA rank parameter\n",
        "    lora_alpha: int = 16              # LoRA alpha scaling parameter\n",
        "    lora_dropout: float = 0.05        # LoRA dropout rate\n",
        "    save_steps: int = 250             # Steps between checkpoint saves\n",
        "    logging_steps: int = 10           # Steps between log outputs\n",
        "    warmup_ratio: float = 0.1         # Warmup ratio for learning\n",
        "\n",
        "def create_training_config(model_name: str, **kwargs) -> TrainingConfig:\n",
        "    \"\"\"Create training configuration with automatic output directory.\"\"\"\n",
        "    # Create clean directory name from model name\n",
        "    model_clean = model_name.split('/')[-1].replace('-', '_').replace('.', '_')\n",
        "    current_struct_time = time.localtime(time.time())\n",
        "    formatted_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", current_struct_time)\n",
        "    default_output_dir = f\"./{model_clean}_{formatted_time}_PPO_xLAM\"\n",
        "\n",
        "\n",
        "    config_dict = {'output_dir': default_output_dir, **kwargs}\n",
        "    return TrainingConfig(**config_dict)\n",
        "def create_lora_config(training_config: TrainingConfig) -> LoraConfig:\n",
        "    \"\"\"\n",
        "    Create LoRA configuration for parameter-efficient fine-tuning.\n",
        "\n",
        "    LoRA (Low-Rank Adaptation) adds small trainable matrices to specific\n",
        "    model layers while keeping the base model frozen.\n",
        "\n",
        "    Args:\n",
        "        training_config (TrainingConfig): Training configuration with LoRA parameters\n",
        "\n",
        "    Returns:\n",
        "        LoraConfig: Configured LoRA adapter settings\n",
        "\n",
        "    LoRA Parameters:\n",
        "        - r (rank): Dimensionality of adaptation matrices (higher = more capacity)\n",
        "        - alpha: Scaling factor for LoRA weights\n",
        "        - dropout: Regularization to prevent overfitting\n",
        "        - target_modules: Which model layers to adapt\n",
        "    \"\"\"\n",
        "    print(\"‚öôÔ∏è Configuring LoRA adapters...\")\n",
        "\n",
        "    # Target modules for both Llama and Qwen architectures\n",
        "    target_modules = [\n",
        "        'k_proj', 'q_proj', 'v_proj', 'o_proj',  # Attention projections\n",
        "        \"gate_proj\", \"down_proj\", \"up_proj\"       # Feed-forward projections\n",
        "    ]\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        lora_alpha=training_config.lora_alpha,\n",
        "        lora_dropout=training_config.lora_dropout,\n",
        "        r=training_config.lora_r,\n",
        "        bias=\"none\",                             # Don't adapt bias terms\n",
        "        task_type=\"CAUSAL_LM\",                   # Causal language modeling\n",
        "        target_modules=target_modules\n",
        "    )\n",
        "\n",
        "    print(f\"üéØ LoRA targeting modules: {target_modules}\")\n",
        "    print(f\"üìä LoRA parameters: r={training_config.lora_r}, alpha={training_config.lora_alpha}\")\n",
        "\n",
        "    return lora_config\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading in Datasets\n",
        "\n",
        "we want to load in the datasets that we have already split into train and eval for this next round of training.\n"
      ],
      "metadata": {
        "id": "GDaYbOr-P6ey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EDWpunnYzDb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_from_disk\n",
        "from datasets import DatasetDict\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = load_from_disk(\"./data/train\")\n",
        "test_dataset = load_from_disk(\"./data/test\")\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\":  test_dataset,\n",
        "})\n",
        "# For GRPO, we want the raw datasets with a \"prompt\" column\n",
        "grpo_train_dataset = dataset_dict[\"train\"]\n",
        "grpo_eval_dataset  = dataset_dict[\"test\"]\n",
        "\n",
        "print(grpo_train_dataset[0].keys())  # should include \"prompt\"\n",
        "\n",
        "sample = grpo_train_dataset[0]\n",
        "print(sample)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hEA0ShJeFU4"
      },
      "source": [
        "# Start training\n",
        "\n",
        "- Now we begin using GRPO to iron out the formatiing\n",
        "\n",
        "\n",
        "## Results from Fine-Tuning Training\n",
        "\n",
        "| Step | Training Loss | Validation Loss | Entropy   | Num Tokens      | Mean Token Accuracy |\n",
        "|------|---------------|-----------------|-----------|------------------|----------------------|\n",
        "| 100  | 0.086300      | 0.080959        | 1.079421  | 5,140,499        | 0.977191             |\n",
        "| 200  | 0.063900      | 0.060436        | 1.038168  | 10,289,647       | 0.983178             |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIPJGDipVOyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dIz-QF5_pDO"
      },
      "source": [
        "## Reward Mechanism Summary\n",
        "\n",
        "The reward function scores how well the model outputs a valid `<calls>...</calls>` block using allowed tools and parameters. The score is kept within `[-1, 1]`.\n",
        "\n",
        "### What Increases Reward\n",
        "- **A valid `<calls>` block is present**\n",
        "- **Tool calls parse correctly** as Python dictionaries\n",
        "- **No hallucinated tools** (all tools are in the `<tools>` list)\n",
        "- **No hallucinated parameters** (arguments match allowed params)\n",
        "- **Covers all user IDs** referenced in `<user>...</user>`\n",
        "- **No extra text** before or after `<calls>` block\n",
        "- **Clean formatting** (minor style bonuses)\n",
        "\n",
        "### What Decreases Reward\n",
        "- Missing `<calls>` block (large penalty)\n",
        "- Extra natural language or random text around the block\n",
        "- Incorrect or unparsable tool calls\n",
        "- Using tools not listed in `<tools>`\n",
        "- Using parameters that aren‚Äôt allowed\n",
        "- Referencing IDs that weren‚Äôt provided by the user\n",
        "- Very long completions\n",
        "\n",
        "### Philosophy\n",
        "- Good structure = strong positive reward  \n",
        "- Mistakes = proportional negative reward  \n",
        "- All rewards are clamped to `[-1, 1]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw7QHs2__rIn"
      },
      "source": [
        "Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPjqmR_BZQ97"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import ast\n",
        "\n",
        "# ===== compile regexes once =====\n",
        "user_block_re  = re.compile(r\"<user>(.*?)</user>\", re.DOTALL)\n",
        "calls_block_re = re.compile(r\"<calls>(.*?)</calls>\", re.DOTALL)\n",
        "tools_block_re = re.compile(r\"<tools>(.*?)</tools>\", re.DOTALL)\n",
        "\n",
        "id_re    = re.compile(r\"[\\\"'](\\d{5,})[\\\"']\")\n",
        "is_id_re = re.compile(r\"[\\\"']is_id[\\\"']\\s*:\\s*[\\\"']([^\\\"']+)[\\\"']\")\n",
        "\n",
        "end_token = tokenizer.eos_token\n",
        "print(\"end token is \", end_token)\n",
        "\n",
        "\n",
        "def _extract_user_ids(text: str):\n",
        "    m = user_block_re.search(text)\n",
        "    if not m:\n",
        "        return set()\n",
        "    return set(id_re.findall(m.group(1)))\n",
        "\n",
        "\n",
        "def _extract_called_ids(calls_text: str):\n",
        "    if not calls_text:\n",
        "        return set()\n",
        "    return set(is_id_re.findall(calls_text))\n",
        "\n",
        "\n",
        "def _safe_parse_python_dict(line: str):\n",
        "    \"\"\"\n",
        "    Safely parse a single Python dict literal using ast.literal_eval.\n",
        "    Returns a dict or None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        obj = ast.literal_eval(line)\n",
        "        if isinstance(obj, dict):\n",
        "            return obj\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _extract_allowed_tools(prompt: str):\n",
        "    \"\"\"\n",
        "    Parse the <tools>...</tools> block in the prompt and return:\n",
        "        { tool_name: {param1, param2, ...}, ... }\n",
        "\n",
        "    Assumes each tool is a Python-style dict on its own line, e.g.:\n",
        "\n",
        "    <tools>{'name': 'locations_v2_list', ...}\n",
        "    {'name': 'detail', ...}\n",
        "    ...</tools>\n",
        "    \"\"\"\n",
        "    m = tools_block_re.search(prompt)\n",
        "    if not m:\n",
        "        return {}\n",
        "\n",
        "    tools_text = m.group(1).strip()\n",
        "    lines = [ln.strip() for ln in tools_text.split(\"\\n\") if ln.strip()]\n",
        "\n",
        "    allowed = {}\n",
        "    for line in lines:\n",
        "        obj = _safe_parse_python_dict(line)\n",
        "        if obj is None:\n",
        "            # skip bad tool lines, but don't kill everything\n",
        "            continue\n",
        "\n",
        "        name = obj.get(\"name\")\n",
        "        params = obj.get(\"parameters\", {})\n",
        "        if name:\n",
        "            allowed[name] = set(params.keys())\n",
        "\n",
        "    return allowed\n",
        "\n",
        "\n",
        "def _parse_calls_body_to_dict_list(calls_body: str):\n",
        "    \"\"\"\n",
        "    Parse the contents of <calls>...</calls> into a list of dict objects.\n",
        "\n",
        "    Assumes the model outputs one Python-style dict per line, e.g.:\n",
        "\n",
        "        {'name': 'shares_float', 'arguments': {'symbol': 'V'}}\n",
        "        {'name': 'stock_balance_sheet_stock', 'arguments': {'symbol': 'MA'}}\n",
        "\n",
        "    Returns:\n",
        "        list[dict] on success,\n",
        "        None if ANY line fails.\n",
        "    \"\"\"\n",
        "    calls_body = calls_body.strip()\n",
        "    if not calls_body:\n",
        "        return []\n",
        "\n",
        "    lines = [ln.strip() for ln in calls_body.split(\"\\n\") if ln.strip()]\n",
        "    parsed = []\n",
        "\n",
        "    for line in lines:\n",
        "        obj = _safe_parse_python_dict(line)\n",
        "        if obj is None:\n",
        "            return None\n",
        "        parsed.append(obj)\n",
        "\n",
        "    return parsed\n",
        "\n",
        "\n",
        "def _compute_single_reward(prompt: str, completion: str, debug: bool = False) -> float:\n",
        "    if debug:\n",
        "        print(\"\\n================ REWARD DEBUG ================\")\n",
        "        print(\"RAW COMPLETION:\")\n",
        "        print(repr(completion))\n",
        "\n",
        "    completion = completion.strip()\n",
        "\n",
        "    # Start from neutral\n",
        "    reward = 0.0\n",
        "\n",
        "    # 0. Handle truly empty completions (hard bottom)\n",
        "    if not completion:\n",
        "        if debug:\n",
        "            print(\"[WARN] Empty completion -> reward = -1.0\")\n",
        "        return -1.0\n",
        "\n",
        "    # 1. Check for <calls>...</calls> block\n",
        "    m = calls_block_re.search(completion)\n",
        "    if not m:\n",
        "        # Big penalty for missing calls block (but not autofail beyond this)\n",
        "        reward -= 0.7\n",
        "        if debug:\n",
        "            print(\"[PENALTY] No <calls>...</calls> block found -> -0.7\")\n",
        "        # Nothing else to score, just clamp and return\n",
        "        return max(min(reward, 1.0), -1.0)\n",
        "\n",
        "    # 2. Check that completion is basically just the calls block (plus optional EOS)\n",
        "    #    Instead of failing, we apply penalties for extra text.\n",
        "    if not (completion.startswith(\"<calls>\") and completion.endswith(\"</calls>\")):\n",
        "        prefix = completion.split(\"<calls>\", 1)[0].strip()\n",
        "        suffix = completion.split(\"</calls>\", 1)[1].strip()\n",
        "\n",
        "        if prefix:\n",
        "            reward -= 0.2\n",
        "            if debug:\n",
        "                print(f\"[PENALTY] Text before <calls> block: {repr(prefix)} -> -0.2\")\n",
        "\n",
        "        # tolerate EOS after </calls>, penalize anything else\n",
        "        if suffix and suffix != end_token:\n",
        "            reward -= 0.2\n",
        "            if debug:\n",
        "                print(f\"[PENALTY] Text after </calls> block: {repr(suffix)} -> -0.2\")\n",
        "\n",
        "    # 3. Penalize if the model tries to answer in natural language style\n",
        "    if \"<response>\" in completion.lower():\n",
        "        reward -= 0.3\n",
        "        if debug:\n",
        "            print(\"[PENALTY] Found <response> tag -> -0.3\")\n",
        "\n",
        "    calls_body = m.group(1)\n",
        "\n",
        "    # 4. Parse calls body\n",
        "    parsed_calls = _parse_calls_body_to_dict_list(calls_body)\n",
        "    if parsed_calls is None:\n",
        "        # Can't parse -> strong negative, but not hard fail\n",
        "        reward -= 0.6\n",
        "        if debug:\n",
        "            print(\"[PENALTY] Calls block parse failed -> -0.6\")\n",
        "            print(\"Calls body:\")\n",
        "            print(calls_body)\n",
        "    else:\n",
        "        if parsed_calls:\n",
        "            # Some valid dicts parsed: give a positive base\n",
        "            reward += 0.6\n",
        "            if debug:\n",
        "                print(\"[REWARD] Parsed calls dicts -> +0.6\")\n",
        "                print(\"Parsed calls:\", parsed_calls)\n",
        "\n",
        "    # 5. Allowed tools / params from prompt\n",
        "    allowed_tools = _extract_allowed_tools(prompt)\n",
        "    if debug:\n",
        "        print(\"\\nALLOWED TOOLS FROM PROMPT:\")\n",
        "        print(allowed_tools)\n",
        "\n",
        "    hallucinated_tools = 0\n",
        "    hallucinated_params = 0\n",
        "\n",
        "    if parsed_calls:\n",
        "        for call in parsed_calls:\n",
        "            tool_name = call.get(\"name\")\n",
        "            args      = call.get(\"arguments\", {})\n",
        "\n",
        "            # Unknown / hallucinated tool\n",
        "            if tool_name not in allowed_tools:\n",
        "                hallucinated_tools += 1\n",
        "                if debug:\n",
        "                    print(f\"[HALLUCINATION] Tool '{tool_name}' not in allowed tools.\")\n",
        "                continue\n",
        "\n",
        "            allowed_params = allowed_tools[tool_name]\n",
        "            for p in args.keys():\n",
        "                if p not in allowed_params:\n",
        "                    hallucinated_params += 1\n",
        "                    if debug:\n",
        "                        print(f\"[HALLUCINATION] Param '{p}' not allowed for tool '{tool_name}'.\")\n",
        "\n",
        "        # Apply penalties for hallucinations\n",
        "        if hallucinated_tools > 0:\n",
        "            penalty = 0.2 * hallucinated_tools\n",
        "            reward -= penalty\n",
        "            if debug:\n",
        "                print(f\"\\n[TOOL HALLUCINATION PENALTY] -{penalty:.3f} for {hallucinated_tools} hallucinated tools.\")\n",
        "                print(f\"Reward now: {reward}\")\n",
        "\n",
        "        if hallucinated_params > 0:\n",
        "            penalty = 0.1 * hallucinated_params\n",
        "            reward -= penalty\n",
        "            if debug:\n",
        "                print(f\"[PARAM HALLUCINATION PENALTY] -{penalty:.3f} for {hallucinated_params} hallucinated params.\")\n",
        "                print(f\"Reward now: {reward}\")\n",
        "\n",
        "        # Small bonus if we have calls and *no* hallucinations\n",
        "        if hallucinated_tools == 0 and hallucinated_params == 0 and len(parsed_calls) > 0:\n",
        "            reward += 0.2\n",
        "            if debug:\n",
        "                print(\"[REWARD] No hallucinated tools/params -> +0.2\")\n",
        "                print(f\"Reward now: {reward}\")\n",
        "\n",
        "    # 6. Old ID coverage logic (soft)\n",
        "    user_ids   = _extract_user_ids(prompt)\n",
        "    called_ids = _extract_called_ids(calls_body)\n",
        "\n",
        "    if debug:\n",
        "        print(\"\\nUSER IDS (from <user> block):\", user_ids)\n",
        "        print(\"CALLED IDS (from <calls> body):\", called_ids)\n",
        "        trimmed_body = calls_body[:300] + (\"...\" if len(calls_body) > 300 else \"\")\n",
        "        print(\"CALLS BODY (trimmed):\")\n",
        "        print(trimmed_body)\n",
        "\n",
        "    if user_ids:\n",
        "        covered = user_ids.intersection(called_ids)\n",
        "        recall = len(covered) / len(user_ids) if user_ids else 0.0\n",
        "\n",
        "        # Scale coverage contribution modestly\n",
        "        coverage_bonus = 0.2 * recall\n",
        "        reward += coverage_bonus\n",
        "        if debug:\n",
        "            print(\"\\n[COVERAGE]\")\n",
        "            print(\"  Covered IDs:\", covered)\n",
        "            print(f\"  Recall = {recall:.3f}\")\n",
        "            print(f\"  +{coverage_bonus:.3f} for coverage.\")\n",
        "            print(f\"Reward now: {reward}\")\n",
        "\n",
        "        if recall == 1.0:\n",
        "            reward += 0.1\n",
        "            if debug:\n",
        "                print(\"  [BONUS] All user IDs covered -> +0.1\")\n",
        "                print(f\"Reward now: {reward}\")\n",
        "\n",
        "        hallucinations = called_ids - user_ids\n",
        "        if hallucinations:\n",
        "            penalty = 0.1 * len(hallucinations)\n",
        "            reward -= penalty\n",
        "            if debug:\n",
        "                print(\"\\n[ID HALLUCINATIONS]\")\n",
        "                print(\"  Hallucinated IDs:\", hallucinations)\n",
        "                print(f\"  -{penalty:.3f} for hallucinated IDs.\")\n",
        "                print(f\"Reward now: {reward}\")\n",
        "\n",
        "    # 7. Light style bonuses (very small)\n",
        "    if \"{\" in calls_body and \"}\" in calls_body:\n",
        "        reward += 0.05\n",
        "        if debug:\n",
        "            print(\"\\n[STYLE] +0.05 for having braces in calls body.\")\n",
        "            print(f\"Reward now: {reward}\")\n",
        "    if \"name\" in calls_body:\n",
        "        reward += 0.05\n",
        "        if debug:\n",
        "            print(\"[STYLE] +0.05 for 'name' in calls body.\")\n",
        "            print(f\"Reward now: {reward}\")\n",
        "\n",
        "    # 8. Length penalty (tiny)\n",
        "    if len(completion) > 600:\n",
        "        reward -= 0.1\n",
        "        if debug:\n",
        "            print(\"\\n[LENGTH] Completion too long (>600 chars) -> -0.1\")\n",
        "            print(f\"Reward now: {reward}\")\n",
        "\n",
        "    # 9. Clamp to [-1, 1]\n",
        "    clamped = max(min(reward, 1.0), -1.0)\n",
        "    if debug:\n",
        "        print(\"\\n[FINAL]\")\n",
        "        print(f\"Unclamped reward: {reward:.3f}\")\n",
        "        print(f\"Clamped reward:   {clamped:.3f}\")\n",
        "        print(\"==============================================\")\n",
        "    return clamped\n",
        "\n",
        "\n",
        "def calls_reward_func(prompts, completions, completion_ids, trainer_state, debug: bool = False, **kwargs):\n",
        "    rewards = []\n",
        "    for prompt, completion in zip(prompts, completions):\n",
        "        r = _compute_single_reward(prompt, completion, debug=debug)\n",
        "        rewards.append(float(r))\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEC7yvhp_xCO"
      },
      "source": [
        "# Creating Training and LoraConfigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzZyQCUPc9vv"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_config: TrainingConfig = create_training_config(MODEL_NAME)\n",
        "print(\"creating training config\", type(training_config))\n",
        "\n",
        "peft_config: LoraConfig = create_lora_config(training_config)\n",
        "print(\"creating peft config\", type(peft_config))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating GRPO Config\n",
        "\n",
        "\n",
        "### Training Basics\n",
        "- **batch_size=32**, **grad_accum=2** ‚Üí effective batch = 64\n",
        "- **epochs=1**, **lr=1e-6** ‚Üí very gentle updates\n",
        "- **bf16=True** ‚Üí faster training, less memory\n",
        "\n",
        "### GRPO-Specific\n",
        "- **beta=0.05** ‚Üí KL penalty strength (keeps model close to base policy)\n",
        "- **num_generations=8** ‚Üí for each prompt, generate 8 candidates ‚Üí compute rewards ‚Üí update policy\n",
        "- **scale_rewards=True** ‚Üí normalizes reward distribution for stability\n",
        "\n",
        "### Logging & Saving\n",
        "- Logs every **10 steps** to W&B  \n",
        "- Saves checkpoint every **50 steps**, keeps last **5**\n",
        "\n",
        "### Data & Length\n",
        "- **max_prompt_length=1024**, **max_completion_length=512**\n",
        "- Uses **3 dataloader workers** + pinned memory ‚Üí faster input pipeline\n",
        "\n",
        "### vLLM Integration\n",
        "- **use_vllm=True**, **mode=\"colocate\"** ‚Üí extremely fast generation for GRPO (trying to speedup generation)\n"
      ],
      "metadata": {
        "id": "x9Yd80P4QuFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "# Use a fresh output dir for GRPO runs\n",
        "grpo_output_dir = training_config.output_dir.replace(\"PPO_xLAM\", \"GRPO_xLAM\")\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=grpo_output_dir,\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-6,\n",
        "    beta = 0.05,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"qwen2-grpo-xlam\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=5,\n",
        "    bf16=True,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_num_workers=3,\n",
        "    dataloader_pin_memory=True,\n",
        "    max_prompt_length=1024,\n",
        "    max_completion_length=512,\n",
        "    num_generations=8,\n",
        "    scale_rewards=True,\n",
        "    use_vllm = True,\n",
        "    vllm_mode = \"colocate\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IpSUlQHvuZAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W80y0XwwxSnE"
      },
      "source": [
        "# Training and Model Saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woI3BGaLdRtN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from trl import PPOConfig, PPOTrainer\n",
        "from datasets import Dataset\n",
        "\n",
        "print(trl.__version__)\n",
        "\n",
        "print(fine_tuned_model.device)\n",
        "print(fine_tuned_model.model.embed_tokens.weight.device)\n",
        "def print_mem(label=\"\"):\n",
        "    alloc = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"[{label}] allocated={alloc:.2f} GB, reserved={reserved:.2f} GB\")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=fine_tuned_model,        # your Qwen SFT checkpoint\n",
        "    args=grpo_config,\n",
        "    reward_funcs=calls_reward_func,\n",
        "    train_dataset=grpo_train_dataset,\n",
        "    eval_dataset=grpo_eval_dataset,\n",
        "    processing_class=tokenizer,    # tokenizer with left padding + pad_token set\n",
        "    peft_config=peft_config,       # your LoRA config from create_lora_config(...)\n",
        ")\n",
        "print_mem(\"after init\")\n",
        "trainer.train()\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I9uESKGiEOEa"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}