{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdovt0hUQg_5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/rl_llms')\n",
        "\n",
        "print('Google Drive mounted and set as current working directory.')\n",
        "!ls\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U trl\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb_api_key = \"ur wandb apikey\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Template Model Config, and Training Configs\n",
        "\n",
        "Inspired by https://huggingface.co/learn/cookbook/en/function_calling_fine_tuning_llms_on_xlam\n",
        "\n",
        "who similarly trained a fine-tuned model on this dataset, we base our configs off of his as a baseline and keep it consistent throughout"
      ],
      "metadata": {
        "id": "q9hlbOyxR_AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate.checkpointing import load\n",
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Qwen2ForCausalLM, PreTrainedModel, BitsAndBytesConfig\n",
        "from typing import Tuple\n",
        "import torch\n",
        "import time\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for model-specific settings.\"\"\"\n",
        "    model_name: str           # HuggingFace model identifier\n",
        "    pad_token: str           # Padding token for the tokenizer\n",
        "    pad_token_id: int        # Numerical ID for the padding token\n",
        "    padding_side: str        # Side to add padding ('left' or 'right')\n",
        "    eos_token: str          # End of sequence token\n",
        "    eos_token_id: int       # End of sequence token ID\n",
        "    vocab_size: int         # Vocabulary size\n",
        "    model_type: str         # Model architecture type\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Configuration for training hyperparameters.\"\"\"\n",
        "    output_dir: str                    # Directory to save model checkpoints\n",
        "    batch_size: int = 16              # Training batch size per device\n",
        "    gradient_accumulation_steps: int = 8  # Steps to accumulate gradients\n",
        "    learning_rate: float = 1e-4       # Learning rate for optimization\n",
        "    max_steps: int = 1000             # Maximum training steps\n",
        "    max_seq_length: int = 2048        # Maximum sequence length\n",
        "    lora_r: int = 16                  # LoRA rank parameter\n",
        "    lora_alpha: int = 16              # LoRA alpha scaling parameter\n",
        "    lora_dropout: float = 0.05        # LoRA dropout rate\n",
        "    save_steps: int = 250             # Steps between checkpoint saves\n",
        "    logging_steps: int = 10           # Steps between log outputs\n",
        "    warmup_ratio: float = 0.1         # Warmup ratio for learning rate\n",
        "\n",
        "def auto_configure_model(model_name: str, custom_pad_token: str | None = None) -> ModelConfig:\n",
        "    \"\"\"\n",
        "    Automatically configure any model by extracting information from its tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model_name: HuggingFace model identifier\n",
        "        custom_pad_token: Custom pad token if model doesn't have one\n",
        "\n",
        "    Returns:\n",
        "        ModelConfig: Complete model configuration\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Loading model configuration: {model_name}\")\n",
        "\n",
        "    # Load tokenizer and model config\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    model_config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "    # Extract basic model info\n",
        "    model_type = getattr(model_config, 'model_type', 'unknown')\n",
        "    vocab_size = getattr(model_config, 'vocab_size', len(tokenizer.get_vocab()))\n",
        "\n",
        "    print(f\"Model: {model_type}, vocab_size: {vocab_size:,}\")\n",
        "\n",
        "    # Get EOS token\n",
        "    eos_token = tokenizer.eos_token\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    if eos_token is None:\n",
        "        raise ValueError(f\"Model '{model_name}' missing EOS token\")\n",
        "\n",
        "    # Get or set pad token\n",
        "    pad_token = tokenizer.pad_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    if pad_token is None:\n",
        "        if custom_pad_token is None:\n",
        "            raise ValueError(f\"Model needs custom_pad_token. Use '<|eot_id|>' for Llama, '<|im_end|>' for Qwen\")\n",
        "\n",
        "        pad_token = custom_pad_token\n",
        "        if pad_token in tokenizer.get_vocab():\n",
        "            pad_token_id = tokenizer.get_vocab()[pad_token]\n",
        "        else:\n",
        "            tokenizer.add_special_tokens({'pad_token': pad_token})\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    print(f\"Configured - pad: '{pad_token}' (ID: {pad_token_id}), eos: '{eos_token}' (ID: {eos_token_id})\")\n",
        "\n",
        "    return ModelConfig(\n",
        "        model_name=model_name,\n",
        "        pad_token=pad_token,\n",
        "        pad_token_id=pad_token_id,\n",
        "        padding_side='left',  # Standard for causal LMs\n",
        "        eos_token=eos_token,\n",
        "        eos_token_id=eos_token_id,\n",
        "        vocab_size=vocab_size,\n",
        "        model_type=model_type\n",
        "    )\n",
        "\n",
        "\n",
        "def create_training_config(model_name: str, **kwargs) -> TrainingConfig:\n",
        "    \"\"\"Create training configuration with automatic output directory.\"\"\"\n",
        "    # Create clean directory name from model name\n",
        "    model_clean = model_name.split('/')[-1].replace('-', '_').replace('.', '_')\n",
        "    current_struct_time = time.localtime(time.time())\n",
        "    formatted_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", current_struct_time)\n",
        "    default_output_dir = f\"./{model_clean}_{formatted_time}_xLAM\"\n",
        "\n",
        "\n",
        "    config_dict = {'output_dir': default_output_dir, **kwargs}\n",
        "    return TrainingConfig(**config_dict)\n",
        "\n",
        "def setup_hardware_config() -> Tuple[torch.dtype, str]:\n",
        "    \"\"\"\n",
        "    Automatically detect and configure hardware-specific settings.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.dtype, str]: compute_dtype and attention_implementation\n",
        "    \"\"\"\n",
        "\n",
        "    compute_dtype = torch.float16\n",
        "    attn_implementation = 'sdpa'  # Scaled Dot Product Attention\n",
        "\n",
        "    print(\"Configuration: float16 + SDPA\")\n",
        "\n",
        "    return compute_dtype, attn_implementation\n",
        "\n",
        "def setup_tokenizer(model_config: ModelConfig) -> AutoTokenizer:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_config.model_name, use_fast = True)\n",
        "    tokenizer.pad_token = model_config.pad_token\n",
        "    tokenizer.pad_token_id = model_config.pad_token_id\n",
        "    tokenizer.padding_side = model_config.padding_side\n",
        "    return tokenizer\n",
        "\n",
        "def create_qwen_model(model_config : ModelConfig, tokenizer : AutoTokenizer, compute_type : torch.dtype, attn_implementation : str, token : str) -> PreTrainedModel:\n",
        "    \"\"\" create a qwen model for fine tuninig\"\"\"\n",
        "    print(f\"create a qwen model : {model_config.model_name}\")\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_config.model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        attn_implementation = attn_implementation,\n",
        "        torch_dtype = compute_dtype,\n",
        "        trust_remote_code=True,\n",
        "        token=token,\n",
        "    )\n",
        "    print(f\"Using device: {model.device}\")\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "sGbAQulj9VVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## xLAM Dataset Processing (Brief)\n",
        "\n",
        "- Loads the **Salesforce/xlam-function-calling-60k** dataset.\n",
        "- Splits into **train/test**.\n",
        "- For each row, builds:\n",
        "  - `prompt`:\n",
        "    - `<user>{query}</user>`\n",
        "    - `<tools>{tool_dict_1}\\n{tool_dict_2}\\n...</tools>`\n",
        "  - `completion`:\n",
        "    - `<calls>{answer_dict_1}\\n{answer_dict_2}\\n...</calls><eos>`\n",
        "- Uses multiprocessing + batched `.map()` for speed.\n",
        "- Saves processed `train` and `test` to disk and returns a `DatasetDict`.\n"
      ],
      "metadata": {
        "id": "9cFg0udMSyI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.load import DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import multiprocessing\n",
        "from datasets import load_dataset, Dataset\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "def process_xlam_sample(row: Dict[str, Any], tokenizer: AutoTokenizer) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Process a single xLAM sample into prompt-completion format.\n",
        "\n",
        "    prompt: <user>...</user>\\n\\n<tools>...</tools>\\n\\n\n",
        "    completion: <calls>...</calls><eos>\n",
        "    \"\"\"\n",
        "    # 1) User query\n",
        "    formatted_query = f\"<user>{row['query']}</user>\\n\\n\"\n",
        "\n",
        "    # 2) Tools\n",
        "    try:\n",
        "        parsed_tools = json.loads(row[\"tools\"])\n",
        "        tools_text = \"\\n\".join(str(tool) for tool in parsed_tools)\n",
        "    except json.JSONDecodeError:\n",
        "        tools_text = str(row[\"tools\"])\n",
        "\n",
        "    formatted_tools = f\"<tools>{tools_text}</tools>\\n\\n\"\n",
        "\n",
        "    # 3) Expected function calls (completion)\n",
        "    try:\n",
        "        parsed_answers = json.loads(row[\"answers\"])\n",
        "        answers_text = \"\\n\".join(str(answer) for answer in parsed_answers)\n",
        "    except json.JSONDecodeError:\n",
        "        answers_text = str(row[\"answers\"])\n",
        "\n",
        "    formatted_answers = f\"<calls>{answers_text}</calls>\"\n",
        "\n",
        "    # 4) Split into prompt + completion\n",
        "    prompt = formatted_query + formatted_tools\n",
        "    completion = formatted_answers + tokenizer.eos_token\n",
        "\n",
        "    # Update row\n",
        "    row[\"prompt\"] = prompt         # input to condition on\n",
        "    row[\"completion\"] = completion # tokens we want to learn to generate\n",
        "    # Optionally keep original fields if you still need them:\n",
        "    row[\"query\"] = formatted_query\n",
        "    row[\"tools\"] = formatted_tools\n",
        "    row[\"answers\"] = completion\n",
        "    return row\n",
        "\n",
        "\n",
        "\n",
        "def load_and_process_xlam_dataset(tokenizer: AutoTokenizer, token : str, sample_size: Optional[int] = None, test_size : float = 0.1, output_dir : str = \"./data\") -> DatasetDict:\n",
        "    \"\"\"\n",
        "    Load and process the complete xLAM dataset for function calling training.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Configured tokenizer for the model\n",
        "        sample_size: Optional number of samples to use (None for full dataset)\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Processed dataset ready for training\n",
        "    \"\"\"\n",
        "    print(\"Loading xLAM function calling dataset...\")\n",
        "\n",
        "    # Load the Salesforce xLAM dataset from Hugging Face\n",
        "    dataset : Dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", split=\"train\", token = token)\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Original dataset size: {len(dataset):,} samples\")\n",
        "    print(f\"Dataset type: {type(dataset)} column_names : {dataset.column_names}\")\n",
        "\n",
        "    # Sample dataset if requested (useful for testing)\n",
        "    if sample_size is not None and sample_size < len(dataset):\n",
        "        dataset = dataset.select(range(sample_size))\n",
        "        print(f\"Using sample size: {sample_size:,} samples\")\n",
        "\n",
        "    # Process all samples using multiprocessing for efficiency\n",
        "    print(\"Processing dataset samples into training format...\")\n",
        "    dataset_dict = dataset.train_test_split(test_size, shuffle = True)\n",
        "    def process_batch(batch):\n",
        "        \"\"\"Process a batch of samples with the tokenizer.\"\"\"\n",
        "        processed_batch = []\n",
        "        for i in range(len(batch['query'])):\n",
        "            row = {\n",
        "                'query': batch['query'][i],\n",
        "                'tools': batch['tools'][i],\n",
        "                'answers': batch['answers'][i],\n",
        "            }\n",
        "            processed_row = process_xlam_sample(row, tokenizer)\n",
        "            processed_batch.append(processed_row)\n",
        "\n",
        "        # Convert to batch format\n",
        "        return {\n",
        "            'query': [item['query'] for item in processed_batch],\n",
        "            'tools': [item['tools'] for item in processed_batch],\n",
        "            'answers': [item['answers'] for item in processed_batch],\n",
        "            'prompt': [item['prompt'] for item in processed_batch],\n",
        "            'completion': [item['completion'] for item in processed_batch]\n",
        "        }\n",
        "\n",
        "    # Process the dataset\n",
        "    train = dataset_dict[\"train\"].map(\n",
        "        process_batch,\n",
        "        batched=True,\n",
        "        batch_size=100,  # Process in batches for efficiency\n",
        "        num_proc=min(4, multiprocessing.cpu_count()),  # Use multiple cores\n",
        "        desc=\"Processing xLAM samples\"\n",
        "    )\n",
        "\n",
        "    test = dataset_dict[\"test\"].map(\n",
        "        process_batch,\n",
        "        batched=True,\n",
        "        batch_size=100,  # Process in batches for efficiency\n",
        "        num_proc=min(4, multiprocessing.cpu_count()),  # Use multiple cores\n",
        "        desc=\"Processing xLAM samples\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Dataset processing complete!\")\n",
        "    print(f\"train dataset size: {len(train):,} samples\")\n",
        "    print(f\"test dataset size: {len(test):,} samples\")\n",
        "    if output_dir is not None:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        train_path = os.path.join(output_dir, \"train\")\n",
        "        test_path = os.path.join(output_dir, \"test\")\n",
        "        print(f\"Saving train dataset to: {train_path}\")\n",
        "        train.save_to_disk(train_path)\n",
        "        print(f\"Saving test dataset to: {test_path}\")\n",
        "        test.save_to_disk(test_path)\n",
        "\n",
        "    return DatasetDict({\n",
        "        'train': train,\n",
        "        'test': test\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "def preview_dataset_sample(dataset: Dataset, index: int = 0) -> Dict[str, Any] | None:\n",
        "    \"\"\"\n",
        "    Display a formatted preview of a dataset sample for inspection.\n",
        "\n",
        "    Args:\n",
        "        dataset: The processed dataset\n",
        "        index: Index of the sample to preview (default: 0)\n",
        "    \"\"\"\n",
        "    if index >= len(dataset):\n",
        "        print(f\"Index {index} is out of range. Dataset has {len(dataset)} samples.\")\n",
        "        return None\n",
        "\n",
        "    sample = dataset[index]\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gdXrt2FsVKlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Lora Configs\n",
        "\n",
        "- **Configure LoRA adapters**  \n",
        "  - `create_lora_config(...)` builds a `LoraConfig` that adds low-rank trainable adapters to key projection, value projection, and feed-forward layers of a causal LM (LLaMA/Qwen style).  \n",
        "  - It uses LoRA hyperparameters stored in `TrainingConfig` (rank `r`, `alpha`, `dropout`).\n",
        "\n",
        "- **Run QLoRA supervised fine-tuning (SFT)**  \n",
        "  - `train_qlora_sft_model(...)` sets up an `SFTTrainer` with:\n",
        "    - 8-bit optimizer (`adamw_8bit`) and gradient checkpointing for memory savings.\n",
        "    - FP16/BF16 mixed precision depending on `compute_dtype`.\n",
        "    - Train/eval datasets from a `DatasetDict` (`\"train\"` / `\"test\"`).\n",
        "    - Logging, saving every N steps, and evaluation every 100 steps.\n",
        "  - Then it calls `trainer.train()` to fine-tune the model with LoRA adapters.\n"
      ],
      "metadata": {
        "id": "otGb3FmdYEnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from trl.trainer.sft_trainer import SFTTrainer\n",
        "from trl.trainer.sft_config import SFTConfig\n",
        "def create_lora_config(training_config: TrainingConfig) -> LoraConfig:\n",
        "    \"\"\"\n",
        "    Create LoRA configuration for parameter-efficient fine-tuning.\n",
        "\n",
        "    LoRA (Low-Rank Adaptation) adds small trainable matrices to specific\n",
        "    model layers while keeping the base model frozen.\n",
        "\n",
        "    Args:\n",
        "        training_config (TrainingConfig): Training configuration with LoRA parameters\n",
        "\n",
        "    Returns:\n",
        "        LoraConfig: Configured LoRA adapter settings\n",
        "\n",
        "    LoRA Parameters:\n",
        "        - r (rank): Dimensionality of adaptation matrices (higher = more capacity)\n",
        "        - alpha: Scaling factor for LoRA weights\n",
        "        - dropout: Regularization to prevent overfitting\n",
        "        - target_modules: Which model layers to adapt\n",
        "    \"\"\"\n",
        "    print(\"âš™ï¸ Configuring LoRA adapters...\")\n",
        "\n",
        "    # Target modules for both Llama and Qwen architectures\n",
        "    target_modules = [\n",
        "        'k_proj', 'q_proj', 'v_proj', 'o_proj',  # Attention projections\n",
        "        \"gate_proj\", \"down_proj\", \"up_proj\"       # Feed-forward projections\n",
        "    ]\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        lora_alpha=training_config.lora_alpha,\n",
        "        lora_dropout=training_config.lora_dropout,\n",
        "        r=training_config.lora_r,\n",
        "        bias=\"none\",                             # Don't adapt bias terms\n",
        "        task_type=\"CAUSAL_LM\",                   # Causal language modeling\n",
        "        target_modules=target_modules\n",
        "    )\n",
        "\n",
        "    print(f\"ðŸŽ¯ LoRA targeting modules: {target_modules}\")\n",
        "    print(f\"ðŸ“Š LoRA parameters: r={training_config.lora_r}, alpha={training_config.lora_alpha}\")\n",
        "\n",
        "    return lora_config\n",
        "\n",
        "def train_qlora_sft_model(dataset_dict: DatasetDict,\n",
        "                      model: AutoModelForCausalLM,\n",
        "                      training_config: TrainingConfig,\n",
        "                      compute_dtype: torch.dtype) -> SFTTrainer:\n",
        "    \"\"\"\n",
        "    Execute QLoRA fine-tuning with comprehensive configuration and monitoring.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): Processed training dataset\n",
        "        model (AutoModelForCausalLM): QLoRA-configured model\n",
        "        training_config (TrainingConfig): Training hyperparameters\n",
        "        compute_dtype (torch.dtype): Computation data type\n",
        "\n",
        "    Returns:\n",
        "        SFTTrainer: Trained model trainer\n",
        "\n",
        "    Training Features:\n",
        "        - Supervised fine-tuning with SFTTrainer\n",
        "        - Memory-optimized settings for consumer GPUs\n",
        "        - Comprehensive logging and checkpointing\n",
        "        - Automatic mixed precision training\n",
        "    \"\"\"\n",
        "    print(\"ðŸš€ Starting QLoRA fine-tuning...\")\n",
        "\n",
        "    # Create LoRA configuration\n",
        "    peft_config = create_lora_config(training_config)\n",
        "    dataset_size = len(dataset_dict)\n",
        "    training_config.output_dir = f\"{training_config.output_dir}_{dataset_size}\"\n",
        "\n",
        "    # Configure training arguments\n",
        "    training_arguments = SFTConfig(\n",
        "        output_dir=training_config.output_dir,\n",
        "        optim=\"adamw_8bit\",                      # 8-bit optimizer for memory efficiency\n",
        "        per_device_train_batch_size=training_config.batch_size,\n",
        "        gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
        "        log_level=\"info\",                        # Detailed logging\n",
        "        save_steps=training_config.save_steps,\n",
        "        logging_steps=training_config.logging_steps,\n",
        "        learning_rate=training_config.learning_rate,\n",
        "        fp16=compute_dtype == torch.float16,     # Use FP16 if not using bfloat16\n",
        "        bf16=compute_dtype == torch.bfloat16,    # Use bfloat16 if supported\n",
        "        max_steps=training_config.max_steps,\n",
        "        warmup_ratio=training_config.warmup_ratio,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        completion_only_loss =  True,\n",
        "        max_length=training_config.max_seq_length,\n",
        "        remove_unused_columns=False,             # Keep all dataset columns\n",
        "\n",
        "        # Additional stability and performance settings\n",
        "        dataloader_drop_last=True,               # Drop incomplete batches\n",
        "        gradient_checkpointing=True,             # Enable gradient checkpointing\n",
        "        save_total_limit=3,                      # Keep only 3 most recent checkpoints\n",
        "        load_best_model_at_end=False,            # Don't load best model (saves memory)\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 100,\n",
        "\n",
        "    )\n",
        "\n",
        "    print(f\"Saving model to {training_arguments.output_dir}\")\n",
        "    # Create trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset_dict[\"train\"],\n",
        "        eval_dataset=dataset_dict[\"test\"],\n",
        "        peft_config=peft_config,\n",
        "        args=training_arguments,\n",
        "    )\n",
        "\n",
        "    print(f\"Training configuration:\")\n",
        "    print(f\"Batch size: {training_config.batch_size}\")\n",
        "    print(f\"Gradient accumulation: {training_config.gradient_accumulation_steps}\")\n",
        "    print(f\"Effective batch size: {training_config.batch_size * training_config.gradient_accumulation_steps}\")\n",
        "    print(f\"Max steps: {training_config.max_steps:,}\")\n",
        "    print(f\"Learning rate: {training_config.learning_rate}\")\n",
        "    print(f\"Output directory: {training_config.output_dir}\")\n",
        "\n",
        "    # Start training\n",
        "    print(\"Beginning training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "    return trainer\n"
      ],
      "metadata": {
        "id": "IEQVhnkQmaJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Configs and Training\n",
        "\n",
        "- **Load configuration + hardware setup**\n",
        "  - Uses `auto_configure_model()` to build model settings for `Qwen2-0.5B-Instruct`.\n",
        "  - Creates a `training_config` with LoRA/QLoRA hyperparameters.\n",
        "  - Detects hardware settings (`compute_dtype`, attention backend).\n",
        "\n",
        "- **Initialize tokenizer + base model**\n",
        "  - `setup_tokenizer()` loads the tokenizer with correct padding and special tokens.\n",
        "  - `create_qwen_model()` loads the Qwen model in the selected precision \\\n",
        "\n",
        "- **Load and preprocess dataset**\n",
        "  - `load_and_process_xlam_dataset()` downloads/loads the xLAM tool-use dataset, tokenizes it, filters & formats it.\n",
        "  - `sample_size=30000` loads 30k examples.\n",
        "  - `preview_dataset_sample()` prints a single processed training entry.\n",
        "\n"
      ],
      "metadata": {
        "id": "nUYZkKbjYglm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "\n",
        "\n",
        "HF_TOKEN = \"Ur token\"\n",
        "MODEL_NAME = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "custom_pad_token = None #\"<|im_end|>\"\n",
        "\n",
        "model_config = auto_configure_model(MODEL_NAME, custom_pad_token)\n",
        "training_config = create_training_config(MODEL_NAME)\n",
        "compute_dtype, attn_implementation = setup_hardware_config()\n",
        "tokenizer : AutoTokenizer = setup_tokenizer(model_config)\n",
        "\n",
        "model = create_qwen_model(model_config, tokenizer, compute_dtype, attn_implementation, HF_TOKEN)\n",
        "from datasets import Dataset\n",
        "dataset_dict = load_and_process_xlam_dataset(tokenizer, HF_TOKEN, sample_size=30000)\n",
        "sample = preview_dataset_sample(dataset_dict[\"train\"], index=0)\n",
        "print(sample)\n",
        "\n"
      ],
      "metadata": {
        "id": "xHlytlTkWyNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample.keys())\n",
        "print(sample['prompt'])\n",
        "print(sample['completion'])"
      ],
      "metadata": {
        "id": "lmnnUUz6EFSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = create_lora_config(training_config)\n",
        "trainer = train_qlora_sft_model(dataset_dict, model, training_config,compute_dtype)\n"
      ],
      "metadata": {
        "id": "e05WEflbdjSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}